Hi-ORS
===============

Hi-ORS 在极端的二元稀疏奖励下变得更简单了：人在环内干预使得每条轨迹最终都是成功的，导致每条轨迹总奖励都是 1 ，跳过拒绝采样直接计算流损失做策略梯度。拒绝采样反而在密集奖励函数上发挥更多功能：即使人在环内干预导致每条轨迹都是成功的，但是每条轨迹总奖励数值不同，通过拒绝采样下限阈值过滤策略，初筛掉一部分轨迹，通过剩下的轨迹通过总奖励高低分配权重比例进行策略优化，也就能理解 Abstract 部分提到的“奖励加权的监督训练”的意思了。

这个方法在设置奖励下限阈值上应该是需要调参的。我猜测，如果奖励下限阈值序列调的数值普遍低，那么 VLA 表现出来的应该是学得快但是子最优，反之如果奖励下限阈值调的数值普遍高，那么 VLA 表现出来可能是学得很慢但是接近最优。

和之前看到过的那篇 action-chunk-ppo 中，使用轨迹长短作为轨迹好坏的指标相比，Hi-ORS 使用轨迹非折扣累计奖励的下限阈值作为“好”轨迹的定义；同时这些被定义为“好”的轨迹被加强训练，同时被阈值限制的轨迹被抑制训练。

Hi-ORS 在训练过程中不强调 SFT 作为 base model 的重要性，但是其能力提升还是在罕见环境下的使用人在环内 correction 数据 BC 。

论文还提到了几点：（1）同时混入旧的数据（不是当前 policy 收集的）一起训练，可以避免模型只沿着新数据方向暴走，从而偏离基础模型太远，稳定训练分布 ==> 这跟那篇 action-chunk-ppo 和 iRe-VLA 中“蓄”的想法近似。（2）为确保训练稳定性，当相对变换的范数低于阈值时，系统会过滤无效 action ，避免初始卡顿，并剔除过短的训练片段以防止动作分块错误。（3）移除短 episodes 过滤器后，成功率降至 60% ，这表明删减无信息量的 rollouts 内容能提升学习稳定性。

.. toctree::
   :maxdepth: 1
   :caption: Contents of Hi-ORS:

   Hi-ORS-1-paper