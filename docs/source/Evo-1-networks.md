# Evo-1 VLA 内部网络结构 分析

Evo-1 获得视觉语言融合 tokens 序列在 `Evo_1\model\internvl3\internvl3_embedder.py` 这个文件中的 `InternVL3Embedder.get_fused_image_text_embedding_from_tensor_images(...)` 函数。

整个 Evo-1 VLA 的动作生成核心模块在 `Evo_1\model\action_head\flow_matching.py` 这个文件中。

**动作生成部分**

**交叉调制的 Diffusion Transformer**

原论文 ***3.2.2. Cross-modulated Diffusion Transformer*** 中提到: "Specifically, the action expert is implemented as a Diffusion Transformer (DiT) [23] that solely relies on stacked cross-attention layers, in contrast to the alternating self-attention and cross-attention structure adopted by prior VLA models."

这句话在 `Evo_1\model\action_head\flow_matching.py` 模块的 `FlowmatchingActionHead.__init__(...)` 函数中的部分体现。

```python
self.transformer_blocks = nn.ModuleList([
    BasicTransformerBlock(
    	embed_dim=embed_dim, 
    	num_heads=num_heads,
    	hidden_dim=embed_dim*4, 
    	dropout=dropout
    ) for _ in range(num_layers)
])
```

其中的 `BasicTransformerBlock` 类具体如下，就是一个标准的 Transformer block，人如其名...

```python
class BasicTransformerBlock(nn.Module):

    def __init__(self, embed_dim: int, num_heads: int, hidden_dim: int, dropout: float = 0.0):
        super().__init__()
        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.ff = nn.Sequential(
            nn.Linear(embed_dim, hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, embed_dim)
        )

    def forward(self, action_tokens: torch.Tensor, context_tokens: torch.Tensor, time_emb: torch.Tensor):
        
        x = self.norm1(action_tokens)  # 前归一化 (batchsize, seq_len, dim)
        # query --> x, key --> context_tokens, value --> context_tokens
        attn_out, _ = self.attn(x, context_tokens, context_tokens)
        x = action_tokens + attn_out  # 残差相加

        x2 = self.norm2(x)  # 前归一化
        if time_emb is not None:
            x2 = x2 + time_emb.unsqueeze(1)  # 时间嵌入, (batchsize, 1, dim), 相加广播机制
        ff_out = self.ff(x2)  # 前馈神经网络
        x = x + ff_out  # 残差相加
        return x  # (batchsize, seq_len, dim)
```

**Ground truth 动作加噪声的过程**

原论文 ***3.2.2. Cross-modulated Diffusion Transformer*** 中提到: "Each noisy action sequence $A_{\tau}^{t}$ is generated by linearly interpolating between the ground-truth
action $A_t$ and a randomly sampled noise vector $\epsilon$: $A_{\tau}^{t} = \tau A_t + (1−\tau)\epsilon$ ."

这句话在 `Evo_1\model\action_head\flow_matching.py` 模块的 `FlowmatchingActionHead.forward(...)` 函数中的开头部分体现。

```python
# ground truth
# 如果存在 horizon: (batchsize, seq_len, action_dim)
# 如果没有 horizon: (batchsize, action_dim)
actions_gt_seq = actions_gt  

noise = torch.rand_like(actions_gt) * 2 - 1  # 将均匀分布的噪声控制在 (-1, 1) 区间
# 如果有动作掩码, 直接对噪声做掩码
if action_mask is not None:
action_mask = action_mask.to(dtype=noise.dtype, device=noise.device)
assert action_mask.shape == noise.shape, f"action_mask shape {action_mask.shape} != noise shape {noise.shape}"
noise = noise * action_mask

if self.horizon > 1:
noise_seq = noise.view(B, self.horizon, self.per_action_dim)
else:  # 如果预测的是单步 action, 则直接填一个维度确保对齐
noise_seq = noise.unsqueeze(1)   # (batchsize, 1, action_dim)

if self.horizon > 1:
t_broadcast = t.view(B, 1, 1)  # (batchsize, seq_len, action_dim)
else:
t_broadcast = t.view(B, 1)

# 广播机制使得维度对齐
action_intermediate_seq = (1 - t_broadcast) * noise_seq + t_broadcast * actions_gt_seq  
```

**Ground truth 动作加噪声的参数调度**

原论文 ***3.2.2. Cross-modulated Diffusion Transformer*** 中提到: "The interpolation weight $\tau$ is sampled from a Beta distribution and clamped to the range $[0.02, 0.98]$ to ensure numerical stability during training."

这句话在 `Evo_1\model\action_head\flow_matching.py` 模块的 `FlowmatchingActionHead.forward(...)` 函数中的开头部分体现。

```python
t = torch.distributions.Beta(2, 2).sample((B,)).clamp(0.02, 0.98).to(device).to(dtype=self.dtype)
```

（1）创建一个 Beta 分布，两个参数 $\alpha=2$ 和 $\beta=2$ ，这个分布在 $(0,1)$ 之间，分布形状对称且中间概率密度较高，两端密度较低。（2）从 `Beta(2,2)` 分布里采样 `(B,)` 也就是 batch_size 个 $(0,1)$ 之间的数；（3）对采样值做裁剪：小于 0.02 的变成 0.02 ，大于 0.98 的变成 0.98 ；（4）最后转换成合适的数据类型和存储设备。

**训练过程中的前向计算**

原论文 ***3.2.2. Cross-modulated Diffusion Transformer*** 中提到: "During training, the action expert is optimized to learn a time-conditioned velocity field $v_\theta$ that drives the interpolated action $A_{\tau}^{t}$ toward the ground-truth action $A_t$ under the multimodal context $z_{t}$ and robot state $s_t$ ."

这句话在 `Evo_1\model\action_head\flow_matching.py` 模块的 `FlowmatchingActionHead.forward(...)` 函数中的后半部分体现。

```python
x = action_tokens  # action_tokens 是上面增加时间关联噪声和掩码的动作
for block in self.transformer_blocks:
    # 每个 block 使用 nn.MultiheadAttention 做注意力计算
    # 每个 block 内部 query/key/value 分别是 x/context_tokens/context_tokens
    x = block(x, context_tokens, time_emb)
x = self.norm_out(x)  # x: (batchsize, seq_len, emb_dim)

#########################################################################################

if self.horizon > 1:
    # 当动作是 chunk 的时候, 通过序列融合将 seq_len 整合掉
    x_flat = x.reshape(B, -1)  # x_flat: (batchsize, seq_len * emb_dim)
	if not hasattr(self, "seq_pool_proj"):
		self.seq_pool_proj = nn.Linear(
        	self.horizon * self.embed_dim, self.embed_dim
        ).to(device)
	x_pooled = self.seq_pool_proj(x_flat)  # x_pooled: (batchsize, emb_dim)
else:
	x_pooled = x.squeeze(1)

#########################################################################################

# pred_velocity: (batchsize, action_dim)
pred_velocity = self.mlp_head(x_pooled, embodiment_id)

return pred_velocity, noise
```

要注意的是 `action_dim = horizon × per_action_dim`

**推理过程中的前向计算**

原论文 ***3.2.2. Cross-modulated Diffusion Transformer*** 中提到: $\hat{A}_t = f_{\text{AE}}(z_t, s_t, A_t^\tau)$ 来生成 action chunk $\hat{A}_t = [\hat{a}_t, \hat{a}_{t+1}, \dots, \hat{a}_{t+H-1}]$

这句话在 `Evo_1\model\action_head\flow_matching.py` 模块的 `FlowmatchingActionHead.get_action(...)` 函数中的后半部分体现。 

```python
N = int(getattr(self.config, "num_inference_timesteps", 32))
dt = 1.0 / N
for i in range(N):
    t = i / N 

    time_index = int(t * 1000)  # 相当于在 1000 步中每隔 32 步推理一次
    # time_emb: [1, emb_dim]
    time_emb = self.time_pos_enc(1000)[:, time_index, :].to(device).squeeze(0)
    # time_emb: [batchsize, 1, emb_dim]
    time_emb = time_emb.unsqueeze(0).repeat(B, 1)

#########################################################################################

    # 对掩码动作进行编码
    if self.horizon > 1 and self.action_encoder is not None:
        action_seq = action_seq * action_mask
        action_tokens = self.action_encoder(action_seq, embodiment_id) 
    else:
    	if hasattr(self, "single_action_proj"):
    		action_tokens = self.single_action_proj(action_seq)  
    	else:
    		self.single_action_proj = nn.Linear(per_action_dim, self.embed_dim).to(device)
    	action_tokens = self.single_action_proj(action_seq)

#########################################################################################

    # 传入 DiT 进行前向传播
    x = action_tokens
    for block in self.transformer_blocks:
        x = block(x, context_tokens, time_emb)
    x = self.norm_out(x)  # (batchsize, horizon, emb_dim)

#########################################################################################

    # 序列池化
    if self.horizon > 1:
        x_flat = x.reshape(B, -1)  # (batchsize, horizon * emb_dim)
        if hasattr(self, "seq_pool_proj"):
            x_pooled = self.seq_pool_proj(x_flat)
        else:
            self.seq_pool_proj = nn.Linear(self.horizon * self.embed_dim, self.embed_dim).to(device)
    		x_pooled = self.seq_pool_proj(x_flat)  # (batchsize, emb_dim)
    else:
    	x_pooled = x.squeeze(1)

#########################################################################################

    # 预测流匹配速度
    pred = self.mlp_head(x_pooled, embodiment_id)    # (batchsize, action_dim_total)

#########################################################################################

    # 将预测的速度作用在 action 上并还原成原始维度
    action = action + dt * pred
    if self.horizon > 1:
    	action_seq = action.view(B, self.horizon, per_action_dim)
    else:
    	action_seq = action.view(B, 1, per_action_dim)

return action
```

**本体 state 和视觉语言融合 tokens 的集成**

原论文 ***3.2.3. Integration Module*** 中提到: "To preserve the complete information from both the perceptual embedding and the robot’s proprioceptive state, <font color=red>we concatenate $z_t$ with the robot state $s_t$</font> instead of projecting them into a shared embedding space."

这句话在 `Evo_1\model\action_head\flow_matching.py` 模块的 `FlowmatchingActionHead.forward(...)` 函数中的开头部分体现。

```python
# ···
context_tokens = fused_tokens  # 来自 VLM 融合 tokens 赋值给 context_tokens
if state is not None and self.state_encoder is not None:
    # 如果前向计算中传入本体维度 且 本体编码器存在的情况下
    # 将本体信息和具身实体编号传入本体编码器, 生成本体嵌入, 嵌入维度和 VLM 融合 tokens 嵌入维度一致
    # state: (batchsize, state_dim), embodiment_id: (batchsize, 1)
    # state_emb: (batchsize, emb_dim)
    state_emb = self.state_encoder(state, embodiment_id)
    state_emb = state_emb.unsqueeze(1)  # state_emb: (batchsize, 1, emb_dim)
    # 然后在 [序列维度] 进行合并, 融合后序列长度 +1, 这个 1 来自于本体
    # context_tokens: (batchsize, seq_len+1, emb_dim)
    context_tokens = torch.cat([context_tokens, state_emb], dim=1) 
# ···
```

**感知-本体融合 token 序列在动作生成中的作用**

原论文 ***3.2.3. Integration Module*** 中提到: "This concatenated feature serves as the <font color=red>key-value input</font> for **the transformer blocks of the action expert**, providing a global and information-preserving context for action generation."

这句话在 `Evo_1\model\action_head\flow_matching.py` 模块的 `FlowmatchingActionHead.forward(...)` 函数中的中间部分体现。

```python
x = action_tokens  # action_tokens 是上面增加时间关联噪声和掩码的动作
for block in self.transformer_blocks:
    # 每个 block 使用 nn.MultiheadAttention 做注意力计算
    # 每个 block 内部 query/key/value 分别是 x/context_tokens/context_tokens
    x = block(x, context_tokens, time_emb)
```

**视觉感知部分**

**图像到视觉 tokens 序列转变**

原论文 ***3.2.1. Vision-Language Backbone*** 中提到: "Each RGB observation $\{I_{t}^{i}\}^{N}_{i=1}$ is resized to $448\times 448$ and passed through a pixel-unshuffle downsampling operation, reducing the number of visual tokens by $4\times$."

在主程序中在这里处理：

```python
pixel_values, num_tiles_list = self._preprocess_images(image_tensors)

if pixel_values.shape[0] == 0:
	print("Warning: No valid images to process after masking.")

vit_embeds = self.model.extract_feature(pixel_values)
fused_embeds = vit_embeds  
```

====> 内部处理过程如下：

```python
def _preprocess_images(
    self,
    image_tensors: List[Union[Image.Image, torch.Tensor]]
) -> (torch.Tensor, List[int]):

    pixel_values_list = []
    for i, image in enumerate(image_tensors):
        if isinstance(image, torch.Tensor):
            image = to_pil_image(image)  # 将张量图片转换成 PIL 图片格式
        tiles = dynamic_preprocess(image, image_size=self.image_size)
        # (T_i, 3, 448, 448)
        tile_tensors = torch.stack([self.transform(t) for t in tiles])
        # [(T_i, 3, 448, 448), (T_i, 3, 448, 448), ...]
        pixel_values_list.append(tile_tensors)
    # (T_i1 + T_i2 + T_i3 + ..., 3, 448, 448)
    pixel_values = torch.cat(pixel_values_list, dim=0).to(dtype=torch.bfloat16, device=self.device)
    # [T_i1, T_i2, T_i3, ...]
    num_tiles_list = [pv.shape[0] for pv in pixel_values_list]

    return pixel_values, num_tiles_list
```

====> 最终在这里实现根据不同的图片大小拆分成 $(448\times 448)$ 的小块：

```python
def dynamic_preprocess(image, min_num=1, max_num=1, image_size=448, use_thumbnail=False):
    orig_width, orig_height = image.size  # 获取原图的宽和高
    aspect_ratio = orig_width / orig_height  # 计算宽高比
    target_ratios = set(  # 根据参数生成 "可选的切割方式"
        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if
        i * j <= max_num and i * j >= min_num)
    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])
    target_aspect_ratio = find_closest_aspect_ratio(  # 选出最接近原图长宽比的切割方式
        aspect_ratio, target_ratios, orig_width, orig_height, image_size)
    target_width = image_size * target_aspect_ratio[0]  # 根据选定策略，把图片调整到合适大小
    target_height = image_size * target_aspect_ratio[1]
    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]
    resized_img = image.resize((target_width, target_height))
    processed_images = []
    for i in range(blocks):
        box = (
            (i % (target_width // image_size)) * image_size,
            (i // (target_width // image_size)) * image_size,
            ((i % (target_width // image_size)) + 1) * image_size,
            ((i // (target_width // image_size)) + 1) * image_size
        )
        split_img = resized_img.crop(box)
        processed_images.append(split_img)
    assert len(processed_images) == blocks
    if use_thumbnail and len(processed_images) != 1:
        # 是否加缩略图: 如果开启 use_thumbnail 除了 tile 之外, 会额外再给模型提供:
        # 一张压缩成 448×448 的全部画面
        thumbnail_img = image.resize((image_size, image_size))
        processed_images.append(thumbnail_img)
    return processed_images
```

**视觉 token 在文本 token 中的插入**

原论文 ***3.2.1. Vision-Language Backbone*** 中提到: "For vision-language fusion, InternVL3-1B inserts patch-level image embeddings into the token sequence by replacing a designated `<img>` placeholder token."

这个直接在 `InternVL3Embedder._build_multimodal_prompt()` 体现出来了：

```python
def _build_multimodal_prompt(
    self,
    num_tiles_list: List[int],
    text_prompt: str
) -> str:

    prompt = ''
    for i in range(len(num_tiles_list)):
        prompt += f"Image-{i+1}: <image>\n"
    prompt += text_prompt.strip()

    IMG_CONTEXT_TOKEN = "<IMG_CONTEXT>"
    IMG_START_TOKEN = "<img>"
    IMG_END_TOKEN = "</img>"

    self.img_context_token_id = self.tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)
    for tile_count in num_tiles_list:
        # 这个 num_image_token 是定值
        token_count = self.model.num_image_token * tile_count
        image_tokens = IMG_START_TOKEN + IMG_CONTEXT_TOKEN * token_count + IMG_END_TOKEN
        prompt = prompt.replace("<image>", image_tokens, 1)

    return prompt
```

也就是最终得到的应该是：

```
Image-1: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT>...</img>
Image-2: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT>...</img>
Image-3: <img><IMG_CONTEXT><IMG_CONTEXT><IMG_CONTEXT>...</img>
# 文本 prompt
```

**视觉 token 的 embedding 向量插入进语言 embedding 中**

原论文 ***3.2.1. Vision-Language Backbone*** 中提到: "The resulting fused sequence is processed by the shared transformer decoder, enabling joint reasoning over visual and linguistic context in a unified embedding space."

这个体现在 `InternVL3Embedder._prepare_and_fuse_embeddings()` ：

**1. tokenizer 编码 & 生成 input_ids + attention_mask**

```python
untruncated_ids = self.tokenizer(prompt, return_tensors="pt").input_ids
true_sequence_length = untruncated_ids.shape[1]
```

先把 `prompt` 字符串通过 tokenizer 转成 token untruncated_ids，得到的 `true_sequence_length` 是转换后**未截断的**真正 token 的长度。

```python
model_inputs = self.tokenizer(
    prompt,
    return_tensors="pt",
    padding='max_length',
    truncation=True,
    max_length=self.max_text_length
).to(self.device)
input_ids = model_inputs["input_ids"]
attention_mask = model_inputs["attention_mask"]
```

再一次编码，但这次是 “填充 `padding='max_length'` + 截断 `truncation=True` ” ，确保序列长度是固定的最大 `self.max_text_length` 长度；`attention_mask` 指明哪些 token 是有效 (1)，哪些是 padding (0)。

**2. 找出所有 `<IMG_CONTEXT>` token 在 input_ids 里的位置**

```python
img_token_mask = (input_ids == self.img_context_token_id)
img_token_locations = torch.where(img_token_mask)[1]
```

`self.img_context_token_id` 是 `<IMG_CONTEXT>` 这个 token 对应的 id。

`img_token_mask` 是一个布尔矩阵 (batch_size, sequence_length)，标记出哪里是 `<IMG_CONTEXT>`。

`img_token_locations` 提取出这些 token 在序列里的索引位置。

这些位置非常重要，因为后面要把视觉 embedding 插到这些位置上。

**3. 得到文本 token 的 embedding **

```python
input_embeds = self.model.language_model.get_input_embeddings()(input_ids).clone()
```

这一步调用 LLM 的输入 embedding layer ，把每个 token id 转成对应的 embedding 。`.clone()` 是为了后面修改这些嵌入，例如替换某些位置时，不会破坏原始值，或避免共享内存问题。

```python
B, N, C = input_embeds.shape
input_embeds = input_embeds.reshape(B * N, C)
input_ids = input_ids.reshape(B * N)
```

这里是把 batch 维度和序列长度维度拉平，便于后续对特定 selected token 位置批量操作。

**<font color=red>4. 把视觉特征 (vit_embeds) 融合进文本 embedding</font>**

```python
selected = (input_ids == self.img_context_token_id)
input_embeds[selected] = vit_embeds.reshape(-1, C)
```

`selected` 是一个布尔向量，标记哪些 token 要被替换，即那些 `<IMG_CONTEXT>` 对应的位置。`vit_embeds.reshape(-1, C)` 是把视觉特征 reshape 成和 token embedding 兼容的形状（同样是 $N \times C$ 结构）。然后把这些位置 (selected) 的输入嵌入全部替换成视觉 embedding。

**5. 调整注意力 mask (attention_mask) — 屏蔽无效图像**

下面这一段是为了解决 “哪些图像是无效 / 被屏蔽 (mask)” 的情况：

```python
tokens_per_tile = self.model.num_image_token  # 每个 tile 用多少个 image token

current_token_idx = 0
for i in range(len(image_mask)):
    num_tiles_for_this_image = num_tiles_list[i]
    num_tokens_for_this_image = num_tiles_for_this_image * tokens_per_tile

    if not image_mask[i]:
        start_idx = img_token_locations[current_token_idx]
        end_idx = start_idx + num_tokens_for_this_image
        attention_mask[0, start_idx:end_idx] = 0

    current_token_idx += num_tokens_for_this_image
```

`tokens_per_tile`：每个图像 tile 在文本里被分配的 image token 数量，这个数量是模型设定的。`num_tiles_for_this_image` × `tokens_per_tile` = 这一张图所有 tile 对应的 token 数。如果 `image_mask[i] == False` 那么该图应该被 mask 掉，就把它对应的所有 token 的 attention mask 置为 0。在模型后续 self-attention 里，这些 token **被忽略** (不参与注意力计算)。

**6. 最后 reshape 回去**

```python
input_embeds = input_embeds.reshape(B, N, C)
```

把之前 flatten 的 embedding 再恢复成 (batch, seq_len, hidden_dim) 的形状。

**VLM 输出的隐藏层状态张量**

原论文 ***3.2.1. Vision-Language Backbone*** 中提到: "To better adapt the pretrained VLM to embodied visuomotor tasks, we retain only the first 14 layers of the language branch, as intermediate layers have been empirically found to exhibit stronger cross-modal alignment between visual and linguistic features [26], making them more effective for visuomotor control."

提取前 14 层在 `InternVL3Embedder.__init__()` 中体现了：

```python
if hasattr(self.model.language_model, 'model'):
    layers = self.model.language_model.model.layers
else:
    layers = self.model.language_model.layers
    
layers = layers[:14]

if hasattr(self.model.language_model, 'model'):
    self.model.language_model.model.layers = torch.nn.ModuleList(layers)
else:
    self.model.language_model.layers = torch.nn.ModuleList(layers)
self.model.language_model.lm_head = torch.nn.Identity()

if hasattr(self.model, "vision_model") and hasattr(self.model.vision_model, "encoder"):
    self.model.vision_model.encoder.gradient_checkpointing = False
```

同时在 `InternVL3Embedder.get_fused_image_text_embedding_from_tensor_images()` 的最后计算出融合的 token 张量：

```python
outputs = self.model.language_model(
    inputs_embeds=inputs_embeds,
    attention_mask=attention_mask,
    output_hidden_states=True,
    return_dict=True,
)
fused_hidden = outputs.hidden_states[-1].to(torch.float32)
```

在调用语言模型时，传入 `output_hidden_states=True`，意味着模型输出中 `outputs.hidden_states` 是一个 **tuple**，包含每层（embedding 层 +每个 Transformer 层）输出的隐藏状态。 根据 Transformers 文档：每一层隐藏状态张量的形状是 `(batch_size, sequence_length, hidden_size)`. 

`outputs.hidden_states` 是一个 tuple，如 `(hidden_states_layer0, hidden_states_layer1, …, hidden_states_last_layer)`。`[-1]` 表示取最后一层（最深的一层 Transformer 输出）的隐藏状态张量。因此 `outputs.hidden_states[-1]` 的形状是 `(batch_size, sequence_length, hidden_size)`。