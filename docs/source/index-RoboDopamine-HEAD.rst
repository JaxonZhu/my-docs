Robo-Dopamine
===============

Robo-Dopamine: General Process Reward Modeling for High-Precision Robotic Manipulation

首先提自己感到高兴，因为自己在做的时候，发现到的问题和论文的表述匹配了。如果是直接归一化时间戳进行监督，会导致归一化数值接近于 0 的时候甚至极小，导致训练不良；如果直接使用 TD 差分 + target-network 进行训练的话，会导致训练损失持续增大，很难收敛。因此作者这里用到了 Hop-based 归一化任务进度，而我后续去看 Optimal Transport 了。

在奖励标注这部分都是围绕 Hop-based 归一化任务进度展开，思考问题和解决问题的点都很直接。关于 semantic trap 的推导具有说服力，而且从公式来看这并非是这篇论文这样设计任务进度而独有的问题：涉及到 "从状态抽取任务进度" + "使用任务进度差值计算过程奖励" 这样的做法都会存在 semantic trap 问题，无关乎任务进度网络模型的规模和归一化方式，可以上升到一种共性问题上。 

离散化是这篇工作的另一个大思路，视频数据处理管线 / 统计任务进度分布从而采取更加平衡的数据集训练采样，这些都在简化工作量。在数据管线中，作者对视频帧几乎是随着帧长度均匀划分的，也就是在视频拆分过程没有任务相关内容注入，这就可能导致每个小段之间动作丰富程度可能不平衡：可能某些小段机械臂只是缓慢移动，另一个小段则完成了移动到按钮 / 按下按钮 并 弹回这些动作，借助 VLM model 实现更任务相关的非均等划分是可行的。

这篇虽然提的是 "奖励模型" / RL, 但是论文没有用到很多数理统计的原理, 反而是很多高数微积分 / 泰勒展开内容.

这篇文章的标题也很有意思, Robo-Dopamine. Dopamine 在建模类脑算法领域就是代表比较细粒度的过程奖励哈哈哈哈.

阅读起来会存在一些小困难, 这篇论文的这个版本中每一小节的变量符号都有一些小差异, 造成阅读上有点前后不一致~


.. toctree::
   :maxdepth: 1
   :caption: Contents of Robo-Dopamine:

   Robo-Dopamine-paper